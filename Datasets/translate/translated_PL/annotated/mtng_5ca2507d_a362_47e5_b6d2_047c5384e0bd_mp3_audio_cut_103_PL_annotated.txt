SP_1: Предположим, что нам удастся собрать, скажем, 300, ну, 300, 500, ну, я не знаю, сколько денег удастся собрать. Эти 500 данных мы можем ввести в GPT-3 и сразу это станет дешевле. Если мы будем пропускать все через GPT-3, то мы сможем оптимизировать эту задачу. Что это такое? У нас есть наша модель, загруженная и настроенная в GPT-3. И мы сразу вводим туда текст без каких-либо подсказок. И сразу получаем JSON и все задачи, которые мы обучили. Другими словами, таким образом мы объединим GPT-3.5 плюс все, что мы сделаем с настройкой. Это также можно сделать с Яндексом, GPT и GigaChat. Я не знаю, возможно ли это. Идея заключается в следующем.
SP_2: Ага. Теперь, где-то потерялся ручка, ну что же.
SP_1: Потом посмотри, как можно развивать эту тему, например, после того, как мы обучили разные модели на GPT-3.5, скажем, gigaChat, Яндекс, Antropiki и т.д., у нас будет много разных резюме. Много разных задач, точнее говоря. Мы можем их потом сократить, отметить те, которые были хорошо обучены, и те, которые дали примерно слабый результат, то есть оценить результат для той же задачи и потом обучить модель критика, который может критиковать и оценивать качество этих задач, и это будет действительно круто, потому что сейчас...
SP_2: А критику тоже нужно будет обучить на основе определенных критериев, не так ли?
SP_1: Да, критику мы тоже возьмем из того же набора данных, но сгенерируем как хорошие, так и плохие метки, то есть наши метки сразу увеличатся в три или четыре раза.
SP_2: Вероятно, сбор от пользователей и самих, ну хорошо, я понимаю.
SP_1: Да, потом идет синхронизация. Именно. А потом мы выравниваем эти метки, например, когда соберем определенное количество пользователей и определим, как будем измерять качество на основе пользователей. Извлекаем эти данные соответственно, и потом критика снова обучается на данных, которые пользователи уже оценили по качеству. Но это уже, когда будет много данных. Понял, да?